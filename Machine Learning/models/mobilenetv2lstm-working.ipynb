{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":10631368,"sourceType":"datasetVersion","datasetId":6582375}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\ntf.random.set_seed(73)\nTPU_INIT = False\n\nif TPU_INIT:\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    except ValueError:\n        raise BaseException('ERROR: Not connected to a TPU runtime!')\nelse:\n    !nvidia-smi\n;\nprint(\"Tensorflow version \" + tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:47.972216Z","iopub.execute_input":"2025-02-01T00:44:47.972543Z","iopub.status.idle":"2025-02-01T00:44:48.212961Z","shell.execute_reply.started":"2025-02-01T00:44:47.972518Z","shell.execute_reply":"2025-02-01T00:44:48.211875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:48.214355Z","iopub.execute_input":"2025-02-01T00:44:48.214635Z","iopub.status.idle":"2025-02-01T00:44:48.219311Z","shell.execute_reply.started":"2025-02-01T00:44:48.214610Z","shell.execute_reply":"2025-02-01T00:44:48.218651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:48.220598Z","iopub.execute_input":"2025-02-01T00:44:48.220866Z","iopub.status.idle":"2025-02-01T00:44:50.013118Z","shell.execute_reply.started":"2025-02-01T00:44:48.220845Z","shell.execute_reply":"2025-02-01T00:44:50.012185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport os.path\nfrom pathlib import Path\n\nDATASET_PATH = Path(\"../input/real-life-violence-situations-dataset/Real Life Violence Dataset\")\n\n# Extract video paths and labels\nvideo_files = list(DATASET_PATH.glob(r\"**/*.mp4\")) + list(DATASET_PATH.glob(r\"**/*.avi\"))\n\n# Check if the path exists and contains videos\nif DATASET_PATH.exists():\n    if video_files:\n        print(f\"Found {len(video_files)} video files (including .mp4 and .avi).\")\n        print(\"Sample files:\")\n        for file in video_files[:5]:  # Show a sample of files\n            print(file)\n    else:\n        print(\"No video files found in the specified path.\")\nelse:\n    print(f\"The path {DATASET_PATH} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.014414Z","iopub.execute_input":"2025-02-01T00:44:50.014666Z","iopub.status.idle":"2025-02-01T00:44:50.059126Z","shell.execute_reply.started":"2025-02-01T00:44:50.014643Z","shell.execute_reply":"2025-02-01T00:44:50.058500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if the path exists\nif not os.path.exists(DATASET_PATH):\n    raise FileNotFoundError(f\"Dataset path is invalid: {DATASET_PATH}\")\n\n# Classes Directories, os.path.join is better than concat '/'\nNonViolenceVideos_Dir = os.path.join(DATASET_PATH, \"NonViolence\")\nViolenceVideos_Dir = os.path.join(DATASET_PATH, \"Violence\")\n\n# Retrieve the list of all the video files present in the Class Directory.\nNonViolence_files_names_list = os.listdir(NonViolenceVideos_Dir)\nViolence_files_names_list = os.listdir(ViolenceVideos_Dir)\n\n# Check if Video Directories Exist\nif not os.path.exists(NonViolenceVideos_Dir) or not os.path.exists(ViolenceVideos_Dir):\n    raise FileNotFoundError(\"One or both class directories are missing!\")\n\n# Ensure the Dataset is Not Empty and contains videos\nif not NonViolence_files_names_list or not Violence_files_names_list:\n    raise ValueError(\"One or both class directories are empty!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.059942Z","iopub.execute_input":"2025-02-01T00:44:50.060140Z","iopub.status.idle":"2025-02-01T00:44:50.066168Z","shell.execute_reply.started":"2025-02-01T00:44:50.060122Z","shell.execute_reply":"2025-02-01T00:44:50.065281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_videos = len(NonViolence_files_names_list) + len(Violence_files_names_list)\nprint(f\"Total Dataset Size: {total_videos} videos\")\nprint(f\"Non-Violence Videos: {len(NonViolence_files_names_list)}\")\nprint(f\"Violence Videos: {len(Violence_files_names_list)}\")\n\n# Print first few file names (head of each directory)\nprint(\"\\nFirst 5 Non-Violence Videos:\")\nprint(\"\\n\".join(NonViolence_files_names_list[:5]))\n\nprint(\"\\nFirst 5 Violence Videos:\")\nprint(\"\\n\".join(Violence_files_names_list[:5]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.067068Z","iopub.execute_input":"2025-02-01T00:44:50.067297Z","iopub.status.idle":"2025-02-01T00:44:50.085229Z","shell.execute_reply.started":"2025-02-01T00:44:50.067268Z","shell.execute_reply":"2025-02-01T00:44:50.084562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom IPython.display import Image, HTML\n\nfrom base64 import b64encode\n\ndef play_video(filepath):\n    html = ''\n    video = open(filepath,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src\n    return HTML(html)\n\n# Randomly select a video file from the Classes Directory.\nRandom_NonViolence_Video = random.choice(NonViolence_files_names_list)\nRandom_Violence_Video = random.choice(Violence_files_names_list)\n\nrandom_nonviolence_video_path = os.path.join(NonViolenceVideos_Dir, Random_NonViolence_Video)\nrandom_violence_video_path = os.path.join(ViolenceVideos_Dir, Random_Violence_Video)\n\nplay_video(random_nonviolence_video_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.133904Z","iopub.execute_input":"2025-02-01T00:44:50.134111Z","iopub.status.idle":"2025-02-01T00:44:50.153546Z","shell.execute_reply.started":"2025-02-01T00:44:50.134093Z","shell.execute_reply":"2025-02-01T00:44:50.152778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_HEIGHT, IMG_WIDTH = 112, 112 # Small objects or intricate actions, increasing it to 224x224\nFRAMES = 10  # Number of frames || sequence per video\nFRAME_SIZE = (IMG_HEIGHT, IMG_WIDTH)  # Target frame size (resize) (IMG_HEIGHT, IMG_WIDTH)\nFRAME_SKIP = 10  # Frame skip for extracting frames\nBATCH_SIZE = 16\nEPOCHS = 20\nLEARNING_RATE = 0.0001\nCOLOR_CHANNELS = 3\nNUM_WORKERS = 8  # Number of parallel workers\nCLASSES_LIST = [\"NonViolence\",\"Violence\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.154662Z","iopub.execute_input":"2025-02-01T00:44:50.154921Z","iopub.status.idle":"2025-02-01T00:44:50.159023Z","shell.execute_reply.started":"2025-02-01T00:44:50.154900Z","shell.execute_reply":"2025-02-01T00:44:50.158063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\ndef normalize_frame(frames):\n    return preprocess_input(frames) # Normalize for MobileNetV2\n\ndef process_video(video_path, label, frame_count=FRAMES, frame_size=FRAME_SIZE):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Get the number of frames in the video\n\n    if total_frames == 0:\n        cap.release()\n        raise ValueError(f\"Video {video_path} has zero frames!\")\n\n    # Calculate the interval after which frames will be added to the list\n    frame_skip = max(1, total_frames // frame_count) # fixed size of frames (FRAME_SKIP) or dynamically, ensure at least 1\n\n    frames = [] # Declare a list to store video frames we will extract\n    extracted_frames = 0 # Track the number of successfully extracted frames\n\n    while cap.isOpened() and extracted_frames < FRAMES:\n        # FRAME_SKIPS condition\n        # frame_id = extracted_frames * frame_skip  # Compute frame index\n\n        # skipping frames to avoid duplications\n        # if frame_id >= total_frames:\n        #     break  # Stop if exceeding total frames\n\n        # cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id) # handles skipping no need to if frame_count % frame_skip == 0:\n\n        success, frame = cap.read()\n        if not success:\n            break  # Stop if frame cannot be read\n        if extracted_frames % frame_skip == 0:\n            frame = cv2.resize(frame, frame_size)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n            # frame = frame / 255.0  # Normalize\n        # IMAGE AUGMENTATION (if needed)\n        # frame = augment_frame(frame)\n\n            frames.append(frame)\n        extracted_frames += 1\n\n    cap.release()\n\n    # Pad with black frames if not enough frames extracted -> can lead to bias\n    while len(frames) < frame_count:\n        # frames.append(np.zeros((*frame_size, 3), dtype=np.float32))\n        frames.append(frames[-1])  # Pad with the last frame\n\n    frames = np.array(frames, dtype=np.float32)\n    frames = normalize_frame(frames)\n    return frames, label  # Shape: (frame_count, 224, 224, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.159874Z","iopub.execute_input":"2025-02-01T00:44:50.160187Z","iopub.status.idle":"2025-02-01T00:44:50.172317Z","shell.execute_reply.started":"2025-02-01T00:44:50.160157Z","shell.execute_reply":"2025-02-01T00:44:50.171546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\nfrom tqdm import tqdm  # Progress bar\n\n# Process videos in parallel using ThreadPoolExecutor for faster processing\n# Use ProcessPoolExecutor for better parallelism with CPU-bound tasks\ndef prepare_data():\n    X, y = [], []\n    with ProcessPoolExecutor(max_workers=8) as executor:\n        results = list(tqdm(executor.map(process_video, video_paths, labels), total=len(video_paths)))\n        for frames, label in results:\n            X.append(frames)\n            y.append(label)\n\n    return np.array(X), np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.173163Z","iopub.execute_input":"2025-02-01T00:44:50.173410Z","iopub.status.idle":"2025-02-01T00:44:50.205326Z","shell.execute_reply.started":"2025-02-01T00:44:50.173390Z","shell.execute_reply":"2025-02-01T00:44:50.204736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_paths = []\nlabels = []\n\nfor video_file in os.listdir(ViolenceVideos_Dir):\n      video_paths.append(os.path.join(ViolenceVideos_Dir, video_file))\n      labels.append(1)  # Violence label\n\nfor video_file in os.listdir(NonViolenceVideos_Dir):\n    video_paths.append(os.path.join(NonViolenceVideos_Dir, video_file))\n    labels.append(0)  # NonViolence label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.206080Z","iopub.execute_input":"2025-02-01T00:44:50.206380Z","iopub.status.idle":"2025-02-01T00:44:50.214319Z","shell.execute_reply.started":"2025-02-01T00:44:50.206350Z","shell.execute_reply":"2025-02-01T00:44:50.213633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom sklearn.model_selection import train_test_split\n\n# Run the processing function\nX, y = prepare_data()\n\n# Print dataset shape\nprint(f\"Dataset Size: {len(X)} videos\")\nprint(f\"X Shape: {X.shape}\")  # Expected: (num_videos, FRAMES, 224, 224, 3)\nprint(f\"y Shape: {y.shape}\")  # Expected: (num_videos,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:44:50.215040Z","iopub.execute_input":"2025-02-01T00:44:50.215292Z","iopub.status.idle":"2025-02-01T00:45:19.049026Z","shell.execute_reply.started":"2025-02-01T00:44:50.215263Z","shell.execute_reply":"2025-02-01T00:45:19.048185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-validation-test split (70-15-15)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n\nprint(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:45:19.049826Z","iopub.execute_input":"2025-02-01T00:45:19.050460Z","iopub.status.idle":"2025-02-01T00:45:20.234121Z","shell.execute_reply.started":"2025-02-01T00:45:19.050433Z","shell.execute_reply":"2025-02-01T00:45:20.233084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Early stopping to prevent overfitting, can monitor val_accuracy\nearly_stopping = EarlyStopping(\n    monitor='val_loss', patience=5, restore_best_weights=True\n)\n\n# Learning rate schedular\n# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1\n)\n\n# Save the best model during training\nmodel_checkpoint = ModelCheckpoint(\n    \"cnn_best_model.keras\", monitor=\"val_loss\", save_best_only=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:45:20.235118Z","iopub.execute_input":"2025-02-01T00:45:20.235424Z","iopub.status.idle":"2025-02-01T00:45:20.261695Z","shell.execute_reply.started":"2025-02-01T00:45:20.235398Z","shell.execute_reply":"2025-02-01T00:45:20.260865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.applications import VGG16, VGG19, inception_v3, InceptionV3, MobileNetV2\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv3D, MaxPooling3D, Conv2D, MaxPool2D, BatchNormalization, MaxPooling2D,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, LeakyReLU, \\\n                        GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, Conv1D, AveragePooling1D, MaxPooling1D\nimport keras\nfrom keras.optimizers import RMSprop, Adam, Optimizer, SGD, AdamW\n\ndef build_MobileNet_model():\n    # Load Pretrained MobileNetV2 (Remove last FC layers)\n    # baseModel = MobileNetV2(pooling='avg', input_tensor=input_tensor)\n    base_model = MobileNetV2(include_top=False , weights=\"imagenet\", input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)) # Specifying Input to match features shape\n\n    # Freeze the layers in the base model\n    base_model.trainable = False\n    \n    # Fine-Tuning to make the last 4 layer trainable\n    for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n    model = Sequential([\n            # Use TimeDistributed to apply the base model to each frame\n            # Passing mobilenet in the TimeDistributed layer to handle the sequence\n            TimeDistributed(base_model, input_shape=(FRAMES, IMG_HEIGHT, IMG_WIDTH, 3)),\n\n            # model.add(Dropout(0.25))\n            # Flatten each frame's output\n            # TimeDistributed(Flatten()),\n            TimeDistributed(GlobalAveragePooling2D()),\n\n            # Apply LSTM for sequence processing\n            # LSTM(64, return_sequences=False),\n            Bidirectional(LSTM(64, return_sequences=False)),\n\n            # model.add(Dropout(0.25))\n\n            # model.add(Dense(256,activation='relu')) # decrease by half till 8 units\n            # model.add(Dropout(0.25))\n\n            # Add Dropout for regularization to prevent overfitting\n            Dropout(0.5),\n\n            # Final Dense layer for classification\n            Dense(len(CLASSES_LIST), activation=\"softmax\")\n        ])\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:45:20.262679Z","iopub.execute_input":"2025-02-01T00:45:20.262977Z","iopub.status.idle":"2025-02-01T00:45:20.271386Z","shell.execute_reply.started":"2025-02-01T00:45:20.262954Z","shell.execute_reply":"2025-02-01T00:45:20.270655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MoBiLSTM_model = build_MobileNet_model()\n\n# MoBiLSTM_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\nMoBiLSTM_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(LEARNING_RATE), metrics=[\"accuracy\"])\n# MoBiLSTM_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n\nMoBiLSTM_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:45:20.272084Z","iopub.execute_input":"2025-02-01T00:45:20.272283Z","iopub.status.idle":"2025-02-01T00:45:22.981223Z","shell.execute_reply.started":"2025-02-01T00:45:20.272266Z","shell.execute_reply":"2025-02-01T00:45:22.980332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MobBiLSTM_model_history = MoBiLSTM_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs = EPOCHS,\n    batch_size = BATCH_SIZE,\n    callbacks = [early_stopping, model_checkpoint, lr_scheduler]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:45:22.982140Z","iopub.execute_input":"2025-02-01T00:45:22.982435Z","iopub.status.idle":"2025-02-01T00:48:00.098269Z","shell.execute_reply.started":"2025-02-01T00:45:22.982410Z","shell.execute_reply":"2025-02-01T00:48:00.097445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MoBiLSTM_model.save(\"mobileNetV2_violence_detection_model.h5\")\n\nfrom tensorflow.keras.models import load_model\n\nloaded_model = load_model(\"mobileNetV2_violence_detection_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T00:48:59.592838Z","iopub.execute_input":"2025-02-01T00:48:59.593168Z","iopub.status.idle":"2025-02-01T00:49:00.751851Z","shell.execute_reply.started":"2025-02-01T00:48:59.593143Z","shell.execute_reply":"2025-02-01T00:49:00.750902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_acc = MobBiLSTM_model_history.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}